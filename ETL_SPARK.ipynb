{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT_OTHER_BODY = {\n",
    "    \"query\": {\n",
    "        \"range\": {\n",
    "            \"@timestamp\": {\n",
    "                \"gte\": 0,\n",
    "                \"lte\": 0\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "LOG_LIST = ('conn', 'dns', 'files', 'http', 'tds', 'tds_sqlbatch', 'ssl')\n",
    "    \n",
    "class ElasticSearch(object):\n",
    "    \n",
    "    def __init__(self, host, port, user, password):\n",
    "        self._els_host = host\n",
    "        self._els_port = port\n",
    "        self._user = user\n",
    "        self._password = password\n",
    "        self._es = None\n",
    "    \n",
    "    def connect(self):\n",
    "        if self._es is None:\n",
    "            elastic = ('http://%s:%s@%s:%s' %(self._user, self._password, self._els_host, self._els_port))\n",
    "            self._es = Elasticsearch([elastic])\n",
    "            \n",
    "    def __ts2utc(self, src):\n",
    "        return 'T'.join(str(datetime.utcfromtimestamp(int(src))).split(' '))\n",
    "    \n",
    "    def __local2utcts(self, src):\n",
    "        return int(str(time.mktime(datetime.strptime(src, '%Y-%m-%d %H:%M:%S').timetuple()))[:-2])\n",
    "        \n",
    "    def extract_data(self, time_start, time_end, index='', cut=10000):\n",
    "        \"\"\"\n",
    "        extract data from elasticsearch and return in json format\n",
    "        input: utc timestamp\n",
    "        \"\"\"\n",
    "        self.connect()\n",
    "        query_body = SELECT_OTHER_BODY\n",
    "        query_body['query']['range'][\"@timestamp\"][\"gte\"] = self.__ts2utc(self.__local2utcts(time_start))\n",
    "        query_body['query']['range']['@timestamp'][\"lte\"] = self.__ts2utc(self.__local2utcts(time_end))\n",
    "        result = self._es.search(index=index, body=query_body, size=cut)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Etl(object):\n",
    "    \"\"\"\n",
    "    extract data from current elasticsearch and save it into spark\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sc):\n",
    "        self.sc = sc\n",
    "        self.spark = SparkSession(sc)\n",
    "#         INIT('ETL Instantiated')\n",
    "        \n",
    "    def run_etl(self, els, start_time, end_time, log_list=LOG_LIST, save=False):\n",
    "        \"\"\"\n",
    "        extract log from els with given index\n",
    "        input format: \"YYYY-MM-DD 00:00:00\"\n",
    "        \"\"\"  \n",
    "        def jsonToDataFrame(json, spark, schema=None):\n",
    "            reader = spark.read\n",
    "            if schema:\n",
    "                reader.schema(schema)\n",
    "            return reader.json(sc.parallelize([json]))\n",
    "        \n",
    "        result = {}\n",
    "        for index in log_list:\n",
    "            es_dict = els.extract_data(\n",
    "                start_time, end_time, index)['hits']['hits']\n",
    "            DF = jsonToDataFrame(json.dumps(es_dict), self.spark)\n",
    "            source = DF.select(\"_source.*\")\n",
    "            if save is True:\n",
    "                source.write.parquet(\"data_test/{}\".format(index), 'overwrite')\n",
    "            result[index] = source\n",
    "            \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "els = ElasticSearch(\"192.168.7.15\", 9200, \"elastic\", \"helios12$\")\n",
    "sc = SparkContext()\n",
    "etl = Etl(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = etl.run_etl(\n",
    "    els, \"2018-01-01 00:00:00\", \"2018-07-17 00:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['conn', 'dns', 'files', 'http', 'tds', 'tds_sqlbatch', 'ssl'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
